{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyORLNl+MQTgB3HB4NZ42BxK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7caaf1c13964ad0b9514a955fd46470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4c6bb20d44942eda11c3d025d33a603",
              "IPY_MODEL_551a63b6c51f4672a2df6e43c1c7e04c",
              "IPY_MODEL_38ff804543d943708e960eb9dcfdfe34"
            ],
            "layout": "IPY_MODEL_7e5532261d334b5aadfd409a44765eb6"
          }
        },
        "d4c6bb20d44942eda11c3d025d33a603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9cb39775dc34959b3a8949331217484",
            "placeholder": "​",
            "style": "IPY_MODEL_b13b263b18ed4cf5b5a0678f790945b5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "551a63b6c51f4672a2df6e43c1c7e04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3124d392393e4bf7acaea53541c78ccf",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eec6fa438f4242f9bd1896fa54d095ed",
            "value": 2
          }
        },
        "38ff804543d943708e960eb9dcfdfe34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ce1f4c0f8b456693b9ddb648b61464",
            "placeholder": "​",
            "style": "IPY_MODEL_1fdee341d82743409caf3e300352e457",
            "value": " 2/2 [00:00&lt;00:00,  2.04it/s]"
          }
        },
        "7e5532261d334b5aadfd409a44765eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9cb39775dc34959b3a8949331217484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13b263b18ed4cf5b5a0678f790945b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3124d392393e4bf7acaea53541c78ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eec6fa438f4242f9bd1896fa54d095ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3ce1f4c0f8b456693b9ddb648b61464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fdee341d82743409caf3e300352e457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Downforcedemon/AI/blob/main/Train_Daneel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers accelerate peft datasets torch torch_xla -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP2J81SmPtfN",
        "outputId": "8eed4413-1cc7-4b3b-b000-34c4ecfe9d65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m61.4/69.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch_xla.core.xla_model as xm\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "ygXG4QtxQybt",
        "outputId": "65a3e6a6-cc25-45c8-e139-cd5ecca2ff0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚀 Step 2: Load and Prepare the Daneel Dialogue Dataset\n",
        "📌 What We Will Do in This Step\n",
        "\n",
        "    Load daneel_dialogue_cleaned.json into memory.\n",
        "    Flatten the structured format (convert the dataset into a simple input-output format).\n",
        "    Prepare data for tokenization by combining dialogues into structured training samples."
      ],
      "metadata": {
        "id": "4D2Gl8NgREmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset path\n",
        "dataset_path = \"/content/daneel_dialogue_cleaned.json\"\n",
        "\n",
        "# Load the cleaned dataset\n",
        "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Flatten the dataset: Extract Q&A pairs\n",
        "training_data = []\n",
        "for entry in raw_data:\n",
        "    context = entry[\"context\"]\n",
        "    for dialogue in entry[\"dialogue\"]:\n",
        "        input_text = dialogue[\"input\"]\n",
        "        output_text = dialogue[\"output\"]\n",
        "\n",
        "        # Format training example as a conversational turn\n",
        "        formatted_sample = {\n",
        "            \"input\": f\"User: {input_text}\\nDaneel: \",\n",
        "            \"output\": output_text\n",
        "        }\n",
        "        training_data.append(formatted_sample)\n",
        "\n",
        "# Print dataset sample\n",
        "print(f\"Total training samples: {len(training_data)}\")\n",
        "print(\"Sample entry:\\n\", training_data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0jyZnm2RTlG",
        "outputId": "15b570b0-c164-482a-f507-54d5153cfe5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 11012\n",
            "Sample entry:\n",
            " {'input': 'User: v1.0 formatting and some spellchecking isaac asimov the robots of dawn doubleday company, inc\\nDaneel: ', 'output': 'garden city, new york by nightfall, inc'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: TPU Strategy and Model Loading Setup"
      ],
      "metadata": {
        "id": "hoZQZ8DBpFRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments"
      ],
      "metadata": {
        "id": "PajdIn4Op4mx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tpu_strategy():\n",
        "  # detect TPU and set up device\n",
        "  device = xm.xla_device()\n",
        "  print(f\"XLA device type: {device}\")\n",
        "\n",
        "  # Set up default TPU parameters\n",
        "  TPU_CORES = 8\n",
        "  BATCH_SIZE = 1\n",
        "  TOTAL_BATCH_SIZE = BATCH_SIZE * TPU_CORES\n",
        "\n",
        "  return device, TPU_CORES, BATCH_SIZE, TOTAL_BATCH_SIZE"
      ],
      "metadata": {
        "id": "4C_HML-7qCpJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name=\"deepseek-ai/deepseek-coder-7b\", device=None):\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...............\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Model Loading with memory optimizations\n",
        "    print(\"Loading model...............\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        use_cache=False,\n",
        "        low_cpu_mem_usage=True,  # Added for memory efficiency\n",
        "        device_map='auto'  # Let the model handle device placement\n",
        "    )\n",
        "\n",
        "    # Enable memory optimizations\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "print(\"\\nInitializing TPU training setup...............\")\n",
        "try:\n",
        "    device, tpu_cores, batch_size, total_batch_size = get_tpu_strategy()\n",
        "    print(f\"TPU Configuration:\\nCores: {tpu_cores}\\nBatch Size per Core: {batch_size}\\nTotal Batch Size: {total_batch_size}\")\n",
        "\n",
        "    model, tokenizer = load_model_and_tokenizer(device=device)\n",
        "    print(\"Model and tokenizer loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during TPU setup: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "ilUQ8mktqox7",
        "outputId": "3a76e475-d3e4-4598-9947-6345d18b89ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "d7caaf1c13964ad0b9514a955fd46470",
            "d4c6bb20d44942eda11c3d025d33a603",
            "551a63b6c51f4672a2df6e43c1c7e04c",
            "38ff804543d943708e960eb9dcfdfe34",
            "7e5532261d334b5aadfd409a44765eb6",
            "c9cb39775dc34959b3a8949331217484",
            "b13b263b18ed4cf5b5a0678f790945b5",
            "3124d392393e4bf7acaea53541c78ccf",
            "eec6fa438f4242f9bd1896fa54d095ed",
            "c3ce1f4c0f8b456693b9ddb648b61464",
            "1fdee341d82743409caf3e300352e457"
          ]
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing TPU training setup...............\n",
            "XLA device type: xla:0\n",
            "TPU Configuration:\n",
            "Cores: 8\n",
            "Batch Size per Core: 4\n",
            "Total Batch Size: 32\n",
            "Loading tokenizer...............\n",
            "Loading model...............\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7caaf1c13964ad0b9514a955fd46470"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Format Traning Data for model input\n",
        "\n",
        "\n",
        "1.   convert text data into tokenized format\n",
        "2.   add proper padding and attention masks\n",
        "3.   ensure all inputs are the right length(512 tokens)\n",
        "4.   Prepare data in Pytorch tensor format\n",
        "\n"
      ],
      "metadata": {
        "id": "wSebzR_Jstaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step converts our training data into a format the model can understand\n",
        "def format_training_data(training_data, tokenizer):\n",
        "  formatted_data = []\n",
        "\n",
        "  for item in training_data:\n",
        "    # Combine input and output with appropriate formatting\n",
        "    full_text = f\"{item['input']}{item['output']}\"\n",
        "\n",
        "    # Tokenize and encode the text\n",
        "    encoded = tokenizer(\n",
        "        full_text,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    formatted_data.append({\n",
        "        \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
        "        \"attention_mask\": encoded[\"attention_mask\"].squeeze()\n",
        "    })\n",
        "  return formatted_data\n",
        "\n",
        "# Format the data\n",
        "print(\"Formatting training data for model input.....................\")\n",
        "formatted_training_data = format_training_data(training_data, tokenizer)\n",
        "print(f\"Formatted {len(formatted_training_data)} training examples\")\n",
        "\n",
        "# Show a sample of formatted data\n",
        "print(\"\\nSample formatted input:\")\n",
        "print(f\"Input IDs: {formatted_training_data[0]['input_ids']}\")\n",
        "print(f\"Attention Mask: {formatted_training_data[0]['attention_mask']}\")\n",
        ""
      ],
      "metadata": {
        "id": "Dx22k59UtBsE",
        "outputId": "875cf524-dfcb-42e9-e29a-f41f1ed784ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting training data for model input.....................\n",
            "Formatted 11012 training examples\n",
            "\n",
            "Sample formatted input:\n",
            "Input IDs: tensor([32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014,\n",
            "        32014, 32014, 32014, 32014, 32014, 32014, 32013,  5719,    25,   353,\n",
            "           16,    13,    15,  4797,  1253,   285,   738, 18642, 31913,   317,\n",
            "           64,   305,   372,   308,   872,   254,  6343,  1458,   280, 23698,\n",
            "         4678, 11259,   333,  2595,    11,  2412,   185,    35,  2145,   282,\n",
            "           25,  8325,  3775,    11,   756,   320,  1175,   457,  2639, 12830,\n",
            "           11,  2412])\n",
            "Attention Mask: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Create DataLoader for TPU Training\n",
        "In this step, we:\n",
        "1. Create a custom Dataset class for our formatted data\n",
        "2. Set up a DataLoader with TPU optimization\n",
        "3. Configure proper batch handling for TPU\n",
        "\n",
        "The DataLoader will:\n",
        "- Handle batching of our training data\n",
        "- Shuffle data for better training\n",
        "- Ensure TPU-compatible data delivery\n",
        "- Manage memory efficiently during training\n"
      ],
      "metadata": {
        "id": "ETzM0zzevqUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch_xla.distributed.parallel_loader as pl"
      ],
      "metadata": {
        "id": "A5KS3KEDv2iE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom Dataset class\n",
        "class DaneelDataset(Dataset):\n",
        "  def __init__(self, formatted_data):\n",
        "    self.data = formatted_data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "        'input_ids': self.data[idx]['input_ids'],\n",
        "        'attention_mask': self.data[idx]['attention_mask']\n",
        "    }\n",
        "\n",
        "# create dataset and dataloader\n",
        "print (\"Creating DataLoader for TPU training...........\")\n",
        "train_dataset = DaneelDataset(formatted_training_data)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=total_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# create TPU specific loader\n",
        "train_loader = pl.MpDeviceLoader(train_loader, device)\n",
        "\n",
        "print(f\"Created DataLoader with {len(train_loader)} batches per device\")"
      ],
      "metadata": {
        "id": "c5EcS1E9wBPx",
        "outputId": "9db275b6-f050-432e-fbd7-41dbe1b569f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating DataLoader for TPU training...........\n",
            "Created DataLoader with 345 batches per device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Training Configuration Setup\n",
        "In this step, we:\n",
        "1. Set up TrainingArguments for the model\n",
        "2. Configure the optimizer with appropriate learning rate\n",
        "3. Enable mixed precision training for TPU\n",
        "4. Set up model checkpointing and logging\n",
        "\n",
        "Key configurations:\n",
        "- Using AdamW optimizer (better than standard Adam for transformers)\n",
        "- Mixed precision training (fp16) for better TPU performance\n",
        "- Regular model saving and logging for monitoring"
      ],
      "metadata": {
        "id": "V7cbmGylxXIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "RH8CT8_nxcaN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/daneel_model\",         # Where to save model\n",
        "    num_train_epochs=3,                  # Number of training epochs\n",
        "    per_device_train_batch_size=1,       # Batch size per TPU core\n",
        "    warmup_steps=500,                    # Number of warmup steps\n",
        "    weight_decay=0.01,                   # Weight decay for regularization\n",
        "    logging_steps=100,                   # Log every X steps\n",
        "    save_steps=500,                      # Save model every X steps\n",
        "    fp16=True,                          # Use mixed precision training\n",
        ")\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=2e-5,                # Learning rate\n",
        "    betas=(0.9, 0.999),    # Adam optimizer parameters\n",
        "    eps=1e-8               # Small constant for numerical stability\n",
        ")\n",
        "\n",
        "print(\"Training configuration initialized\")\n",
        "print(f\"Training for {training_args.num_train_epochs} epochs\")\n",
        "print(f\"Saving model to {training_args.output_dir}\")\n"
      ],
      "metadata": {
        "id": "hy-4To8gxfor",
        "outputId": "3896209e-3ea2-4d51-8790-ce0a88201bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration initialized\n",
            "Training for 3 epochs\n",
            "Saving model to /content/daneel_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Training loop\n",
        "\n",
        "\n",
        "Handles batches of data\n",
        "Computes loss\n",
        "Updates model weights\n",
        "Shows progress with tqdm\n",
        "Saves regular checkpoints\n",
        "\n",
        "\n",
        "Main training loop:\n",
        "\n",
        "Runs for specified number of epochs\n",
        "Saves model after each epoch\n",
        "Handles interruptions gracefully\n",
        "Tracks and displays training time\n",
        "\n",
        "\n",
        "TPU-specific optimizations:\n",
        "\n",
        "Uses xm.optimizer_step for TPU\n",
        "Proper device placement\n",
        "TPU-compatible model saving"
      ],
      "metadata": {
        "id": "_L4pb-VZyDkF"
      }
    }
  ]
}