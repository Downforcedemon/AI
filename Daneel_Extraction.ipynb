{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Downforcedemon/AI/blob/main/Daneel_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLyNmIaay71q"
      },
      "source": [
        "## **Step 1: Setup & Install Required Libraries**\n",
        "- Install PyMuPDF for PDF extraction\n",
        "- Install spaCy for Named Entity Recognition (NER)\n",
        "- Load the English NLP model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LWBTb3fy71u",
        "outputId": "9f3005f3-9658-498a-8843-8a4dcd74dc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf spacy\n",
        "import fitz  # PyMuPDF\n",
        "import spacy\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcvfbvziy71x"
      },
      "source": [
        "## **Step 2: Extract Text from All Books**\n",
        "- Loop through all PDFs in the dataset\n",
        "- Extract text and save as `.txt` files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTUwiBWny71x",
        "outputId": "c0381fcf-374f-4f3c-d0ba-68036a72366f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/6_The_Currents_of_Space.pdf\n",
            "Extracted text saved to: /content/6_the_currents_of_space.txt\n",
            "\n",
            "Processing: /content/2_The_Naked_Sun.pdf\n",
            "Extracted text saved to: /content/2_the_naked_sun.txt\n",
            "\n",
            "Processing: /content/3. The_Robots_of_Dawn.pdf\n",
            "Extracted text saved to: /content/3._the_robots_of_dawn.txt\n",
            "\n",
            "Processing: /content/1_The_Caves_Steel.pdf\n",
            "Extracted text saved to: /content/1_the_caves_steel.txt\n",
            "\n",
            "Processing: /content/5_The_Stars_Like_Dust.pdf\n",
            "Extracted text saved to: /content/5_the_stars_like_dust.txt\n",
            "\n",
            "Processing: /content/4_Robtos_And_Empire.pdf\n",
            "Extracted text saved to: /content/4_robtos_and_empire.txt\n",
            "\n",
            "Processing: /content/7_Pebble_in_th_sky.pdf\n",
            "Extracted text saved to: /content/7_pebble_in_th_sky.txt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pdf_files = glob.glob('/content/*.pdf')\n",
        "for pdf_path in pdf_files:\n",
        "    print(f'Processing: {pdf_path}')\n",
        "    doc = fitz.open(pdf_path)\n",
        "    extracted_text = ''\n",
        "    for page in doc:\n",
        "        extracted_text += page.get_text() + '\\n\\n'\n",
        "    book_name = os.path.basename(pdf_path).replace('.pdf', '').replace(' ', '_').lower()\n",
        "    output_text_file = f'/content/{book_name}.txt'\n",
        "    with open(output_text_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(extracted_text)\n",
        "    print(f'Extracted text saved to: {output_text_file}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bckTkrnoy710"
      },
      "source": [
        "## **Step 3: Preprocess Extracted Text**\n",
        "- Remove extra spaces and symbols\n",
        "- Normalize text\n",
        "- Split text into sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VkOR5Gl5y710"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")  # Remove newlines\n",
        "    text = \" \".join(text.split())  # Remove excessive spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    # Corrected regex for removing unwanted characters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:'\\\"()\\\\-]\", \" \", text)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmiA4we5y711"
      },
      "source": [
        "## **Step 4: Named Entity Recognition (NER) & Extract Conversations**\n",
        "- Identify all mentions of Daneel and related keywords\n",
        "- Extract full conversations surrounding mentions\n",
        "- Save the structured data in JSON format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuD_8Obwy712",
        "outputId": "154496eb-1f64-416c-edfe-cffa558de3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing NER for: /content/1_the_caves_steel.txt\n",
            "Extracted conversations saved to /content/daneel_conversations_1_the_caves_steel.json\n",
            "\n",
            "Processing NER for: /content/2_the_naked_sun.txt\n",
            "Extracted conversations saved to /content/daneel_conversations_2_the_naked_sun.json\n",
            "\n",
            "Processing NER for: /content/5_the_stars_like_dust.txt\n",
            "Extracted conversations saved to /content/daneel_conversations_5_the_stars_like_dust.json\n",
            "\n",
            "Processing NER for: /content/3._the_robots_of_dawn.txt\n",
            "Extracted conversations saved to /content/daneel_conversations_3._the_robots_of_dawn.json\n",
            "\n",
            "Processing NER for: /content/7_pebble_in_th_sky.txt\n",
            "Extracted conversations saved to /content/daneel_conversations_7_pebble_in_th_sky.json\n",
            "\n",
            "Processing NER for: /content/6_the_currents_of_space.txt\n",
            "Extracted conversations saved to /content/daneel_conversations_6_the_currents_of_space.json\n",
            "\n",
            "Processing NER for: /content/4_robtos_and_empire.txt\n",
            "Extracted conversations saved to /content/daneel_conversations_4_robtos_and_empire.json\n",
            "\n"
          ]
        }
      ],
      "source": [
        "daneel_keywords = [\n",
        "    'daneel', 'r. daneel olivaw', 'daneel olivaw', 'ol daneel',\n",
        "    'positronic brain', 'humanoid robot', 'spacers robot',\n",
        "    'earth’s first robot', 'the immortal guardian', 'fastolfe’s creation',\n",
        "    'gaia’s protector', 'robot detective', 'robotic partner', 'friend of Elijah Baley'\n",
        "]\n",
        "\n",
        "text_files = glob.glob('/content/*.txt')\n",
        "for text_file_path in text_files:\n",
        "    print(f'Processing NER for: {text_file_path}')\n",
        "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
        "        extracted_text = file.read()\n",
        "    cleaned_text = preprocess_text(extracted_text)\n",
        "    doc = nlp(cleaned_text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    extracted_conversations = []\n",
        "    i = 0\n",
        "    while i < len(sentences):\n",
        "        sentence = sentences[i].lower()\n",
        "        if any(keyword in sentence for keyword in daneel_keywords):\n",
        "            conversation = []\n",
        "            start = max(i - 2, 0)\n",
        "            end = min(i + 5, len(sentences))\n",
        "            for j in range(start, end):\n",
        "                conversation.append(sentences[j])\n",
        "            extracted_conversations.append({\n",
        "                'context': f'Conversation mentioning Daneel (sentence {i})',\n",
        "                'dialogue': ' '.join(conversation)\n",
        "            })\n",
        "            i = end\n",
        "        else:\n",
        "            i += 1\n",
        "    book_name = os.path.basename(text_file_path).replace('.txt', '')\n",
        "    json_output_path = f'/content/daneel_conversations_{book_name}.json'\n",
        "    with open(json_output_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(extracted_conversations, json_file, indent=4)\n",
        "    print(f'Extracted conversations saved to {json_output_path}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_kxBf6ty714"
      },
      "source": [
        "## **Step 5: Preparing Data for AI Training**\n",
        "- The extracted JSON files will now be formatted into a dialogue dataset\n",
        "- Next, we will create an AI training-friendly structure"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load all extracted JSON files\n",
        "json_files = glob.glob(\"/content/daneel_conversations_*.json\")\n",
        "all_dialogues = []\n",
        "\n",
        "# Load all conversations from extracted JSON files\n",
        "for file in json_files:\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "        all_dialogues.extend(data)\n",
        "\n",
        "print(f\"Loaded {len(all_dialogues)} conversations from {len(json_files)} books.\")\n",
        "\n",
        "# Step 2: Convert raw text into structured dialogue format\n",
        "formatted_dataset = []\n",
        "\n",
        "# Function to generate synthetic questions\n",
        "def generate_synthetic_question(response):\n",
        "    prompts = [\n",
        "        \"What does this mean?\", \"Can you elaborate?\", \"Why is this important?\",\n",
        "        \"How does this relate to robots?\", \"Explain further.\", \"What is your perspective on this?\"\n",
        "    ]\n",
        "    return random.choice(prompts)\n",
        "\n",
        "# Process each conversation\n",
        "for conversation in all_dialogues:\n",
        "    dialogue_text = conversation[\"dialogue\"].split(\". \")  # Split conversation into sentences\n",
        "\n",
        "    structured_conversation = {\"context\": conversation[\"context\"], \"dialogue\": []}\n",
        "\n",
        "    for i in range(len(dialogue_text) - 1):\n",
        "        input_text = dialogue_text[i].strip()\n",
        "        output_text = dialogue_text[i + 1].strip()\n",
        "\n",
        "        # Ensure valid Q&A pairs\n",
        "        if input_text and output_text:\n",
        "            structured_conversation[\"dialogue\"].append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "        # Generate synthetic questions if needed\n",
        "        if random.random() < 0.3:  # 30% chance to add a synthetic Q&A\n",
        "            structured_conversation[\"dialogue\"].append({\"input\": generate_synthetic_question(output_text), \"output\": output_text})\n",
        "\n",
        "    formatted_dataset.append(structured_conversation)\n",
        "\n",
        "# Step 3: Save dataset in AI training format\n",
        "json_output_path = \"/content/daneel_dialogue_dataset.json\"\n",
        "with open(json_output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(formatted_dataset, json_file, indent=4)\n",
        "\n",
        "# Provide download link\n",
        "json_output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "S59KoRk7Lql7",
        "outputId": "5649c476-d68b-4a13-8446-cc6ff8e65708"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1343 conversations from 7 books.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/daneel_dialogue_dataset.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}