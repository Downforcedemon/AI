{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYv94V7Pa9jGNE/gkKk9C7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Downforcedemon/AI/blob/main/NLP_dataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Tokenization (Splitting Text into Words or Sentences) process of breaking text into smaller units (words or sentences)."
      ],
      "metadata": {
        "id": "bKhzMO4dwI8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "msRKP2nbwMVP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is amazing! It helps machines understand human language.\"\n",
        "\n",
        "# Tokenizing words\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", words)\n",
        "\n",
        "# Tokenizing sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHYXfJrowWKv",
        "outputId": "f42a6d63-73da-43c7-e4ee-6a7e5dd9a18a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'It', 'helps', 'machines', 'understand', 'human', 'language', '.']\n",
            "Sentence Tokenization: ['Natural Language Processing (NLP) is amazing!', 'It helps machines understand human language.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Stopwords Removal (Filtering Out Common Words)\n",
        "\n",
        "Stopwords are common words (e.g., \"the\", \"is\", \"in\", \"and\") that don‚Äôt add much meaning."
      ],
      "metadata": {
        "id": "4zNogEnUwpnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "words = [\"this\", \"is\", \"an\", \"example\", \"of\", \"text\", \"processing\"]\n",
        "fitered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "print(\"Filtered Words:\", fitered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB3emZFOwtmy",
        "outputId": "e52993b6-8791-4d69-c5fc-ef30079519fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['example', 'text', 'processing']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Stemming (Reducing Words to Root Form)\n"
      ],
      "metadata": {
        "id": "9fERfO8VxE4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"running\",\"easily\",\"fairly\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNtV7Uy0xIVL",
        "outputId": "8787a044-7b0b-4c6b-e4aa-b98ceeebbe20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['run', 'run', 'run', 'easili', 'fairli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Lemmatization (Getting Base Form of Words)\n",
        "\n",
        "Lemmatization converts words to their dictionary form (lemma), considering meaning."
      ],
      "metadata": {
        "id": "LFj7E2obxXY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"flies\", \"better\", \"wolves\"]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLaAabnzxbLl",
        "outputId": "a97e0114-c2d4-48eb-eed0-ff0249fbfb28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['running', 'fly', 'better', 'wolf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Part-of-Speech (POS) Tagging (Identifying Word Types)\n",
        "\n",
        "POS tagging labels words as noun, verb, adjective, etc.."
      ],
      "metadata": {
        "id": "NmDChsqn3Jr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download the necessary data package\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = word_tokenize(\"John plays football on Sunday.\")\n",
        "pos_tags = nltk.pos_tag(text)\n",
        "\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2VDfuw_3Lyt",
        "outputId": "ebfd8d65-31bc-476b-c149-1662e40062b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('John', 'NNP'), ('plays', 'VBZ'), ('football', 'NN'), ('on', 'IN'), ('Sunday', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Named Entity Recognition (NER) (Extracting Names, Locations, etc.)\n",
        "\n",
        "NER identifies important entities like people, places, dates.\n",
        "\n",
        "üìå Example: NER using SpaCy"
      ],
      "metadata": {
        "id": "OG3ezax63sf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Elon Musk founded Tesla in 2003.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HPsD5PV3tGA",
        "outputId": "4334df64-dfb0-4ed0-a730-d066379fe636"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk PERSON\n",
            "Tesla ORG\n",
            "2003 DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Basic NLP Functions\n",
        "Function\tPurpose\tExample Output\n",
        "Tokenization\tSplits text into words/sentences\t['Natural', 'Language', 'Processing', 'is', 'fun', '!']\n",
        "Stopword Removal\tRemoves common words\t['example', 'text', 'processing']\n",
        "Stemming\tReduces words to root form\t'running' ‚Üí 'run'\n",
        "Lemmatization\tConverts words to base dictionary form\t'wolves' ‚Üí 'wolf'\n",
        "POS Tagging\tLabels words as noun, verb, etc.\t[('John', 'NNP'), ('plays', 'VBZ')]\n",
        "NER\tExtracts people, places, organizations\t'Elon Musk' ‚Üí PERSON"
      ],
      "metadata": {
        "id": "aJqJbANH3wbH"
      }
    }
  ]
}